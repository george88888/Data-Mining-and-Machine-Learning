{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing_and_KnnClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/george88888/Data-Mining-and-Machine-Learning/blob/master/Preprocessing_and_KnnClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMmEOTTWNkuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWhTarH6N3YH",
        "colab_type": "code",
        "outputId": "68b43934-8a82-4701-9a9f-78f1dea27bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X = numpy.random.randn(10, 5)\n",
        "print (X[1,:])\n",
        "print (X[:, 1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1.12294062 -0.80681231  0.31560445 -0.78557627 -0.22120653]\n",
            "[ 0.03869315 -0.80681231 -0.2519436   0.76538903 -0.77842728  0.03151713\n",
            " -0.87469014  1.21958643  0.00160522 -0.65089622]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T132aok__UQ_",
        "colab_type": "text"
      },
      "source": [
        "Next, lets $\\ell_1$ normalize each feature vector. For this purpose we must compute the sum of the absolute values in each feature vector and divide each element in the vector by the norm. $\\ell_1$ norm is defined as follows:\n",
        "\n",
        "$\\ell_1 (\\mathbf{x}) = \\sum_i |x_i|$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNWVPT-hOWTG",
        "colab_type": "code",
        "outputId": "91549bf4-f223-43a9-8da9-914ff4cdfb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for i in range(0, 10):\n",
        "    print(i, numpy.sum(numpy.abs(X[i,:])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3.2824188176477787\n",
            "1 3.252140177582553\n",
            "2 3.560915337841582\n",
            "3 4.862090959989583\n",
            "4 5.146237148136559\n",
            "5 3.802954397693445\n",
            "6 1.985144187627578\n",
            "7 2.8028333252051434\n",
            "8 3.4798872965928997\n",
            "9 4.186833213615184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQek5eAO_URI",
        "colab_type": "text"
      },
      "source": [
        "Now lets compute $\\ell_2$ norms instead. We need to compute the squares, add them and take the square root for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnC6ErPBOazO",
        "colab_type": "code",
        "outputId": "e35172d6-6e12-41f4-b917-b079b433f121",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for i in range(0, 10):\n",
        "    print (i, numpy.sqrt(numpy.sum(X[i,:] * X[i,:])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.8528285194395113\n",
            "1 1.636340522170592\n",
            "2 1.8374226321634468\n",
            "3 2.559671686352667\n",
            "4 2.4797014822793795\n",
            "5 2.2694374988295802\n",
            "6 1.0627060065452505\n",
            "7 1.6401301448474421\n",
            "8 2.765051110749676\n",
            "9 2.005030185450553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuCM2eE__URR",
        "colab_type": "text"
      },
      "source": [
        "If you wanted to $\\ell_2$ normalize X then this can be done as follows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLvyimQdTTFY",
        "colab_type": "code",
        "outputId": "750840dc-df72-429a-a7a7-8da02a10fd99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "def row_normalize(X):\n",
        "  for i in range(0,10):\n",
        "      norm = numpy.sqrt(numpy.sum(X[i,:] * X[i,:]))\n",
        "      X[i,:] = X[i,:] / norm\n",
        "  return X\n",
        "\n",
        "norm_X = normalize(X)\n",
        "print(norm_X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 6.53994472e-01  2.08832885e-02  6.22333734e-01  4.26978716e-01\n",
            "  -4.73816229e-02]\n",
            " [-6.86251181e-01 -4.93058933e-01  1.92872112e-01 -4.80081167e-01\n",
            "  -1.35183678e-01]\n",
            " [ 1.05015072e-01 -1.37117939e-01  4.78411260e-01  6.18734926e-01\n",
            "   5.98715511e-01]\n",
            " [-7.27246499e-01  2.99018439e-01 -4.56703168e-01 -4.16079941e-01\n",
            "   4.49841023e-04]\n",
            " [ 6.75195013e-01 -3.13919754e-01  5.48650051e-01  2.73925735e-01\n",
            "  -2.63654881e-01]\n",
            " [ 2.93119659e-01  1.38876409e-02  2.80306937e-01 -8.92771595e-01\n",
            "   1.95639716e-01]\n",
            " [-4.51529519e-01 -8.23078191e-01  1.72479341e-01 -1.97680099e-01\n",
            "  -2.23241665e-01]\n",
            " [ 6.09333114e-01  7.43591251e-01 -2.14194931e-02 -6.88969915e-02\n",
            "   2.65668248e-01]\n",
            " [-9.77914004e-01  5.80538926e-04 -2.01398455e-01 -3.53797258e-02\n",
            "  -4.32527643e-02]\n",
            " [ 3.07649705e-01 -3.24631631e-01  6.85616230e-01  5.14053367e-01\n",
            "  -2.56213745e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE4497H9WQY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,10):\n",
        "  row_norm = (numpy.sqrt(numpy.sum(X[i,:] * X[i,:])))\n",
        "  assert row_norm >= (1. - 1e-8) or row_norm <= (1. + 1e-8) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf0lwCir_URl",
        "colab_type": "text"
      },
      "source": [
        "Let us assume that we further wish to scale each feature (dimension) to [0,1] range using (x - min) / (max - min) method (see the lecture notes for details). We need to find the min and max for each feature across all feature vectors. This turns out to be computing the min and max for each column in X. Guess what, numpy has min and max functions that return the min and max values of an array. How convenient... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiSnuw0Pc8df",
        "colab_type": "code",
        "outputId": "5bb21551-9c9c-421f-8671-6fc028aa2b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "for j in range(0, 5):\n",
        "    minVal = numpy.min(X[:,j])\n",
        "    maxVal = numpy.max(X[:,j])\n",
        "    for i in range(0, 10):\n",
        "        X[i,j] = (X[i,j] - minVal) / (maxVal - minVal)\n",
        "        \n",
        "print (X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.98717535 0.53869786 0.94460175 0.8731357  0.25078929]\n",
            " [0.1764329  0.21065022 0.56864593 0.27303252 0.14897451]\n",
            " [0.6550863  0.43784619 0.8186103  1.         1.        ]\n",
            " [0.15163398 0.71623062 0.         0.31537519 0.30625439]\n",
            " [1.         0.32499417 0.88009818 0.77187714 0.        ]\n",
            " [0.76887468 0.53423256 0.64518742 0.         0.53259551]\n",
            " [0.31842091 0.         0.55079386 0.45986669 0.04686294]\n",
            " [0.96015877 1.         0.38105251 0.54506851 0.61380021]\n",
            " [0.         0.52573868 0.22349679 0.56724325 0.25557709]\n",
            " [0.77766421 0.31815682 1.         0.93074356 0.0086287 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a8BMSmpdJjC",
        "colab_type": "text"
      },
      "source": [
        "OK! Everything is in [0,1] now. One thing to remember is that **if min and max are the same then the division during the scaling will be illegal**. If this is the case then **it means all values of that feature are the same**. So you can **either set it to 0 or 1**, as you wish as long as it is consistent. Of course, if a feature has the same value across all train instances then it is not a useful feature because it does not discriminate the different classes. So you can even remove that feature from your train data and be happy about it (one less feature to worry about).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZJTlmOwdmuj",
        "colab_type": "text"
      },
      "source": [
        "Let us assume that we wanted to do Gaussain scaling (see lecture notes) on this X. Here, we would use (x - mean) / sd, where sd is the standard deviation of the feature values. Not very surprisingly numpy has numpy.mean and numpy.std functions that do exactly this. I guess at this point I can convince you why you should use python+numpy for data mining and machine learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGqbxKV7de9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for j in range(0, 5):\n",
        "    mean = numpy.mean(X[:,j])\n",
        "    sd = numpy.std(X[:,j])\n",
        "    for i in range(0, 10):\n",
        "        X[i,j] = (X[i,j] - mean) / sd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtBVu85IefZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XifFhvNuefqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjvXsZ8dejFq",
        "colab_type": "text"
      },
      "source": [
        "# K-Nearest Neighbor Classification\n",
        "\n",
        "With the data normalization primer, we now move onto using these preprocessing steps to pass inputs to a classifier that can discriminate between two classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylHmwoDQeb2u",
        "colab_type": "text"
      },
      "source": [
        "This example shows how to use k-NN classifier to classify a dataset. We will first generate a binary classification dataset consisitng of 2D feature vectors, randomly sampled from two Gaussian distributions. We will then learn a k-NN classifier to separate the two classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIsmwwMsedfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 5\n",
        "pos = numpy.random.multivariate_normal([0,0], [[1,0],[0,1]], 2 * N)\n",
        "neg = numpy.random.multivariate_normal([2,2], [[1,0],[0,1]], 2 * N)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRvg02lAe8pF",
        "colab_type": "code",
        "outputId": "f72c1c29-7ec2-4007-9782-02c5dbd77118",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.scatter(neg[:,0], neg[:,1], c='r')\n",
        "plt.scatter(pos[:,0], pos[:,1], c='b')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f7dd1db0898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBD4AJOL/AZaqKtm8nYH5AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWC7LWuVfRps",
        "colab_type": "text"
      },
      "source": [
        "We now split the positive and negative samples into N training samples and N test samples (i.e 50-50 split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orqzKAmKfXrv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos = pos[:N,:]\n",
        "test_pos = pos[N:,:]\n",
        "train_neg = neg[:N,:]\n",
        "test_neg = neg[:N,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcAD-BlffrZU",
        "colab_type": "text"
      },
      "source": [
        "Lets assign pos (+1) and neg (-1) labels to our train and test instances.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzB1860-fnvm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = [(1, x) for x in train_pos]\n",
        "train_data.extend([(-1, x) for x in train_neg])\n",
        "\n",
        "test_data = [(1, x) for x in test_pos]\n",
        "test_data.extend([(-1, x) for x in test_neg])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN-WUicYf5Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (train_data)\n",
        "print (test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2mB_o0LgDaX",
        "colab_type": "text"
      },
      "source": [
        "Below we implement a k-NN function that uses the cosine similarity function compute the similarity scores between k-neighbouring samples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56nSssYef-ZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sim(p, q):\n",
        "    score = numpy.dot(p,q) / (numpy.linalg.norm(p) * numpy.linalg.norm(q))\n",
        "    return score\n",
        "\n",
        "def predict(x, k):\n",
        "    L = [(y, sim(x, z)) for (y,z) in train_data]\n",
        "    L.sort(key=lambda tup: tup[1], reverse = True)  # sorts in place    \n",
        "    #print L[:k]\n",
        "    score = sum([e[0] for e in L[:k]])\n",
        "    if score > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu-50HKQgi8G",
        "colab_type": "text"
      },
      "source": [
        "Take a moment to study the predict function. k-NN happens here. We are given k and the instance to be classified, x. The first thing we do is computing the similarity scores between x and each instance z in our train dataset. We must also store the labels so that we can later find the majority label.\n",
        "\n",
        "Next, we need to find the neighbours. For that we sort this list of tuples by the value of the second item in tuples, which is similarity. lambda expressions are convenient ways to write in-place functions. Here, we take two elements from our list, compare their similarity scores and return -1 or +1. The sort function will then use this to sort the list. In this case, it will sort in the descending order of similarity scores.\n",
        "\n",
        "If you would like to confirm that it is indeed the descending order you can print the list after sorting (uncomment that line).\n",
        "\n",
        "Next, we must find the majority label. Since we are doing binary classification and our labels are -1 and +1, when we add the labels for the nearest neigbours if we get a positive value then there must be more +1s than -1s, vice versa. You might have to do more complicated stuff for finding the majority label if there were more than 2 classes. But it is easy for the binary case as shown here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_kD43hRgvGi",
        "colab_type": "text"
      },
      "source": [
        "Lets compute the accuracy of our k-NN classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfgtTGtigvt5",
        "colab_type": "code",
        "outputId": "4123d6ff-0beb-4b87-f5ec-5f85f83b6c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corrects = 0\n",
        "k = 5\n",
        "for (y,x) in test_data:\n",
        "    if y == predict(x, k):\n",
        "        corrects += 1\n",
        "accuracy = float(corrects) / float(len(test_data))\n",
        "print (\"Accuracy =\", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWoB_tlonitK",
        "colab_type": "text"
      },
      "source": [
        "So we have a decent? classifier here. Try the following things:\n",
        "* change the value of k\n",
        "* increase the number of instances N\n",
        "* separate or bring together the two classes by adjusting the means of the two Gaussians.\n",
        "\n",
        "How does the accuracy vary in each case?"
      ]
    }
  ]
}